{
 "cells": [
  {
   "source": [
    "## Martin Dionne"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Face Recognition, but not evil this time\n",
    "\n",
    "Using the faces dataset in:\n",
    "\n",
    "```\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "```\n",
    "\n",
    "If you use the `faces.target` and `faces.target_names` attributes, you can build a facial recognition algorithm.\n",
    "\n",
    "Use sklearn **gridsearch** (or an equivalent, like random search) to optimize the model for accuracy. Try both a SVM-based classifier and a logistic regression based classifier (with a feature pipeline of your choice) to get the best model. You should have at least 80% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "#faces = fetch_lfw_people(min_faces_per_person=60, resize=0.4, color=False)\n",
    "\n",
    "X = faces.data\n",
    "y = faces.target\n",
    "labels = faces.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "#X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=5)]: Done 118 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=5)]: Done 278 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=5)]: Done 502 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:   55.5s\n",
      "[Parallel(n_jobs=5)]: Done 1142 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done 1558 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=5)]: Done 1920 out of 1920 | elapsed:  2.8min finished\n",
      "{'pca__n_components': 75, 'pca__whiten': True, 'svm__C': 100000.0, 'svm__gamma': 0.01, 'svm__kernel': 'rbf'}\n",
      "0.8562211981566821\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "pipe_svc = Pipeline([\n",
    "    ('pca', PCA()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "param_grid_svc = [{\n",
    "    'pca__whiten': [True, False],\n",
    "    'pca__n_components': [50, 75, 100, 175],\n",
    "    'svm__kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "    'svm__C': [5e4, 1e5, 5e5, 1e6],\n",
    "    'svm__gamma': [0.001, 0.005, 0.01, 0.1]\n",
    "}]\n",
    "\n",
    "grid_svc = GridSearchCV(pipe_svc, param_grid_svc, \n",
    "                        scoring='accuracy',\n",
    "                        cv=5,\n",
    "                        n_jobs=5,\n",
    "                        verbose=3)\n",
    "\n",
    "grid_svc.fit(X_train, y_train)\n",
    "print(grid_svc.best_params_)\n",
    "print(grid_svc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 4  1  1  0  1  0  0]\n [ 0 41  0  3  1  0  0]\n [ 0  1 10  4  0  0  1]\n [ 2  0  1 75  0  0  0]\n [ 0  0  1  3 10  0  0]\n [ 0  0  0  1  0 11  0]\n [ 0  0  0  1  1  0 18]]\n              precision    recall  f1-score   support\n\n           0       0.67      0.57      0.62         7\n           1       0.95      0.91      0.93        45\n           2       0.77      0.62      0.69        16\n           3       0.86      0.96      0.91        78\n           4       0.77      0.71      0.74        14\n           5       1.00      0.92      0.96        12\n           6       0.95      0.90      0.92        20\n\n    accuracy                           0.88       192\n   macro avg       0.85      0.80      0.82       192\nweighted avg       0.88      0.88      0.88       192\n\n"
     ]
    }
   ],
   "source": [
    "grid_svc_pred = grid_svc.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, grid_svc_pred))\n",
    "print(classification_report(y_test, grid_svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=5)]: Done 118 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=5)]: Done 278 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=5)]: Done 320 out of 320 | elapsed:   27.8s finished\n",
      "{'lgr__C': 0.0005, 'lgr__fit_intercept': False, 'pca__n_components': 250, 'pca__whiten': False}\n",
      "0.8562211981566821\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "pipe_lgr = Pipeline([\n",
    "    ('pca', PCA()),\n",
    "    ('lgr', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid_lgr = [{\n",
    "    'pca__whiten': [True, False],\n",
    "    'pca__n_components': [100, 150, 250, 300],\n",
    "    'lgr__fit_intercept': [True, False],\n",
    "    'lgr__C': [1e-6, 5e-5, 1e-4, 0.0005]\n",
    "}]\n",
    "\n",
    "grid_lgr = GridSearchCV(pipe_lgr, param_grid_lgr, \n",
    "                        scoring='accuracy',\n",
    "                        cv=5,\n",
    "                        n_jobs=5,\n",
    "                        verbose=3)\n",
    "\n",
    "grid_lgr.fit(X_train, y_train)\n",
    "print(grid_lgr.best_params_)\n",
    "print(grid_svc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 4  1  1  0  1  0  0]\n [ 1 38  1  3  0  1  1]\n [ 2  3 10  1  0  0  0]\n [ 2  3  2 64  3  1  3]\n [ 0  0  0  2 12  0  0]\n [ 0  0  0  1  0 11  0]\n [ 0  1  0  0  1  1 17]]\n              precision    recall  f1-score   support\n\n           0       0.44      0.57      0.50         7\n           1       0.83      0.84      0.84        45\n           2       0.71      0.62      0.67        16\n           3       0.90      0.82      0.86        78\n           4       0.71      0.86      0.77        14\n           5       0.79      0.92      0.85        12\n           6       0.81      0.85      0.83        20\n\n    accuracy                           0.81       192\n   macro avg       0.74      0.78      0.76       192\nweighted avg       0.82      0.81      0.81       192\n\n"
     ]
    }
   ],
   "source": [
    "grid_lgr_pred = grid_lgr.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, grid_lgr_pred))\n",
    "print(classification_report(y_test, grid_lgr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Bag of Words, Bag of Popcorn\n",
    "\n",
    "By this point, you are ready for the [Bag of Words, Bag of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) competition. \n",
    "\n",
    "Use NLP feature pre-processing (using, SKLearn, Gensim, Spacy or Hugginface) to build the best classifier you can. Use a  feature pipeline, and gridsearch for your final model.\n",
    "\n",
    "A succesful project should get 90% or more on a **holdout** dataset you kept for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/labeledTrainData.tsv.zip', delimiter=\"\\t\", quoting=3)\n",
    "y = df['sentiment']\n",
    "X = df['review']\n",
    "raw_review = df['review']\n",
    "\n",
    "import string\n",
    "X = X.str.replace('<[^<]+?>', '', regex=True)\n",
    "#X = df['review'].str.lower().str.replace('[{}]'.format(string.punctuation),'')\n",
    "#X = X.str.replace('[\\W]+', ' ', regex=True).str.lower()\n",
    "X = X.str.replace('[^a-zA-Z]', ' ', regex=True).str.lower()\n",
    "X = X.str.replace('  ', ' ', regex=True)\n",
    "#X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "#X_test, X_holdout, y_test, y_holdout = train_test_split(X_test, y_test, test_size=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class Lemmatizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return  X.apply(lambda text: \" \".join([self.lemmatizer.lemmatize(word) for word in text.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class Doc2Vec(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp, doc='doc'):\n",
    "        self.doc = doc\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.doc == 'word':\n",
    "            # return a vector for each word\n",
    "            # need more work\n",
    "            vec = X.apply(lambda text: np.array([self.nlp(w).vector for w in text]) )\n",
    "        elif self.doc == 'sentense':\n",
    "            # return a vector for each sentense\n",
    "            # need more work\n",
    "            vec = X.apply(lambda text: np.array([self.nlp(s).vector for s in text.split('.')]) )\n",
    "        else:\n",
    "            # return a vector for the whole doc \n",
    "            vec = X.apply(lambda text: np.array(self.nlp(text).vector))\n",
    "            #vec = np.array([self.nlp(text).vector for text in X])\n",
    "        return  pd.DataFrame(vec.values.tolist(), index=vec.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   33.3s remaining:   50.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   33.5s finished\n",
      "{}\n",
      "0.8918588235294116\n"
     ]
    }
   ],
   "source": [
    "# Fastest Model\n",
    "pipe_lsvc = Pipeline([\n",
    "    ('lemma', Lemmatizer()),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    #('norm', Normalizer()),\n",
    "    #('d2v', Doc2Vec(nlp)),\n",
    "    #('mnb', MultinomialNB())\n",
    "    #('svc', SVC())\n",
    "    ('lsvc', LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid_lsvc = [{\n",
    "    ##'tfidf__max_df': [0.92],\n",
    "    ##'tfidf__min_df': [2],\n",
    "    ##'tfidf__max_features': [6000],\n",
    "    #'mnb__alpha': [0.5, 0.8, 1],\n",
    "    ##'svc__kernel': ['rbf'],\n",
    "    ##'svc__C': [0.75],\n",
    "}]\n",
    "\n",
    "grid_lsvc = GridSearchCV(pipe_lsvc, param_grid_lsvc, \n",
    "                    scoring='accuracy', \n",
    "                    cv=5,\n",
    "                    n_jobs=5,\n",
    "                    verbose=3)\n",
    "\n",
    "grid_lsvc.fit(X_train, y_train)\n",
    "print(grid_lsvc.best_params_)\n",
    "print(grid_lsvc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1631  222]\n [ 161 1736]]\n              precision    recall  f1-score   support\n\n           0       0.91      0.88      0.89      1853\n           1       0.89      0.92      0.90      1897\n\n    accuracy                           0.90      3750\n   macro avg       0.90      0.90      0.90      3750\nweighted avg       0.90      0.90      0.90      3750\n\n"
     ]
    }
   ],
   "source": [
    "grid_pred_lsvc = grid_lsvc.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, grid_pred_lsvc))\n",
    "print(classification_report(y_test, grid_pred_lsvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  30 out of  30 | elapsed: 122.4min finished\n",
      "{'svm__C': 5, 'svm__kernel': 'rbf'}\n",
      "0.8967529411764705\n"
     ]
    }
   ],
   "source": [
    "# Most Accurate Model\n",
    "pipe_svm = Pipeline([\n",
    "    ('lemma', Lemmatizer()),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    #('norm', Normalizer()),\n",
    "    #('d2v', Doc2Vec(nlp)),\n",
    "    #('pca', PCA())\n",
    "    #('mnb', MultinomialNB())\n",
    "    ('svm', SVC())\n",
    "    #('lsvc', LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid_svm  = [{\n",
    "    #'tfidf__max_df': [0.92],\n",
    "    #'tfidf__min_df': [2],\n",
    "    #'tfidf__max_features': [6000],\n",
    "    ##'pca__whiten': [True, False],\n",
    "    ##'pca__n_components': [100, 200, 300],\n",
    "    'svm__kernel': ['poly', 'rbf'],\n",
    "    'svm__C': [5, 10, 15],\n",
    "}]\n",
    "\n",
    "grid_svm = GridSearchCV(pipe_svm, param_grid_svm, \n",
    "                    scoring='accuracy', \n",
    "                    cv=5,\n",
    "                    n_jobs=5,\n",
    "                    verbose=3)\n",
    "\n",
    "grid_svm.fit(X_train, y_train)\n",
    "print(grid_svm.best_params_)\n",
    "print(grid_svm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1645  208]\n [ 162 1735]]\n              precision    recall  f1-score   support\n\n           0       0.91      0.89      0.90      1853\n           1       0.89      0.91      0.90      1897\n\n    accuracy                           0.90      3750\n   macro avg       0.90      0.90      0.90      3750\nweighted avg       0.90      0.90      0.90      3750\n\n"
     ]
    }
   ],
   "source": [
    "grid_pred_svm = grid_svm.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, grid_pred_svm))\n",
    "print(classification_report(y_test, grid_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed: 76.8min\n",
      "[Parallel(n_jobs=5)]: Done  45 out of  45 | elapsed: 138.9min finished\n",
      "{'lbgm__boosting_type': 'goss', 'lbgm__max_depth': -1, 'lbgm__n_estimators': 400, 'lbgm__num_leaves': 35}\n",
      "0.8483294117647059\n"
     ]
    }
   ],
   "source": [
    "# Last try\n",
    "pipe_lgbm = Pipeline([\n",
    "    #('lemma', Lemmatizer()),\n",
    "    #('tfidf', TfidfVectorizer()),\n",
    "    ('d2v', Doc2Vec(nlp)),\n",
    "    #('norm', Normalizer()),\n",
    "    ('lbgm', lgbm.LGBMClassifier())\n",
    "])\n",
    "\n",
    "param_grid_lgbm = [{\n",
    "    #'tfidf__max_df': [0.92],\n",
    "    #'tfidf__min_df': [2],\n",
    "    #'tfidf__max_features': [6000],\n",
    "    'lbgm__boosting_type' : ['goss'], \n",
    "    'lbgm__num_leaves': [15, 25, 35],\n",
    "    'lbgm__max_depth': [-1],\n",
    "    'lbgm__n_estimators': [200, 300, 400]\n",
    "}]\n",
    "\n",
    "grid_lgbm = GridSearchCV(pipe_lgbm, param_grid_lgbm, \n",
    "                    scoring='accuracy', \n",
    "                    cv=5,\n",
    "                    n_jobs=5,\n",
    "                    verbose=3)\n",
    "\n",
    "grid_lgbm.fit(X_train, y_train)\n",
    "print(grid_lgbm.best_params_)\n",
    "print(grid_lgbm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1549  304]\n [ 258 1639]]\n              precision    recall  f1-score   support\n\n           0       0.86      0.84      0.85      1853\n           1       0.84      0.86      0.85      1897\n\n    accuracy                           0.85      3750\n   macro avg       0.85      0.85      0.85      3750\nweighted avg       0.85      0.85      0.85      3750\n\n"
     ]
    }
   ],
   "source": [
    "grid_pred_lgbm = grid_lgbm.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, grid_pred_lgbm))\n",
    "print(classification_report(y_test, grid_pred_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}